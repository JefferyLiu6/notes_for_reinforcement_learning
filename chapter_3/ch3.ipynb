{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Include MathJax for rendering LaTeX equations -->\n",
    "<script type=\"text/javascript\" async\n",
    "  src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML\">\n",
    "</script>\n",
    "\n",
    "# Chapter 3: Finite Markov Decision Processes (MDPs)\n",
    "\n",
    "## Learning Objectives\n",
    "- Define MDPs and their core components: **states**, **actions**, **rewards**, **dynamics function**, and **policies**.\n",
    "- Understand the **Bellman equations** for value functions and optimality.\n",
    "- Differentiate between **episodic** and **continuing tasks** and calculate **returns** with/without discounting.\n",
    "- Explain how **optimal policies** and **value functions** are derived.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Agent-Environment Interface  \n",
    "**MDPs** formalize sequential decision-making where actions affect both immediate rewards and future states.\n",
    "\n",
    "### Key Components:\n",
    "- **Agent**: Learner/decision-maker.\n",
    "- **Environment**: Everything outside the agent.\n",
    "- **State (\\(S_t\\))**: Representation of the environment at time \\(t\\).\n",
    "- **Action (\\(A_t\\))**: Choice made by the agent.\n",
    "- **Reward (\\(R_{t+1}\\))**: Immediate feedback from the environment.\n",
    "- **Dynamics Function (\\(p\\))**:\n",
    "  $$\n",
    "  p(s', r \\mid s, a) \\doteq \\Pr\\{S_t = s', R_t = r \\mid S_{t-1} = s, A_{t-1} = a\\}\n",
    "  $$\n",
    "  Describes the probability of transitioning to state \\(s'\\) with reward \\(r\\) after taking action \\(a\\) in state \\(s\\).\n",
    "\n",
    "### Example: Recycling Robot  \n",
    "- **States**: `high` (battery level), `low`.  \n",
    "- **Actions**: `search`, `wait`, `recharge`.  \n",
    "- **Rewards**:  \n",
    "  - Positive for collecting cans.  \n",
    "  - \\(-3\\) if battery depletes.  \n",
    "- **Dynamics**: Transition probabilities depend on current state and action (e.g., searching with `high` battery has probability \\(\\alpha\\) to stay `high`).\n",
    "\n",
    "---\n",
    "\n",
    "## 3.2 Goals and Rewards  \n",
    "### Reward Hypothesis  \n",
    "> *All goals can be framed as maximizing cumulative reward.*\n",
    "\n",
    "- **Reward Signal**: Immediate feedback (\\(R_{t+1}\\)).  \n",
    "- **Value Function**: Long-term expected return (\\(v_\\pi(s)\\) or \\(q_\\pi(s, a)\\)).\n",
    "\n",
    "**Example**:  \n",
    "- Chess: +1 for win, \\(-1\\) for loss, 0 otherwise.  \n",
    "- Pole-balancing: \\(-1\\) on failure, 0 otherwise.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.3 Returns and Episodes  \n",
    "### Returns  \n",
    "- **Episodic Tasks**: Finite time steps (e.g., a game).  \n",
    "  $$\n",
    "  G_t = R_{t+1} + R_{t+2} + \\cdots + R_T\n",
    "  $$\n",
    "- **Continuing Tasks**: Infinite horizon (e.g., robot operation).  \n",
    "  $$\n",
    "  G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n",
    "  $$\n",
    "  where \\(\\gamma \\in [0, 1]\\) is the **discount factor**.\n",
    "\n",
    "### Key Insight:\n",
    "- **Discounting** prioritizes immediate rewards over delayed ones.\n",
    "\n",
    "**Example**:  \n",
    "If \\(\\gamma = 0.9\\) and \\(R = [2, 7, 7, \\ldots]\\), then  \n",
    "$$\n",
    "G_0 = 2 + 0.9 \\times \\frac{7}{1 - 0.9} = 65.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 Unified Notation  \n",
    "- **Absorbing State**: Terminal state for episodic tasks with 0 reward thereafter.  \n",
    "- Unified return formula:\n",
    "  $$\n",
    "  G_t = \\sum_{k=t+1}^T \\gamma^{k-t-1} R_k\n",
    "  $$\n",
    "  Handles both episodic (\\(T < \\infty\\)) and continuing (\\(T = \\infty\\), \\(\\gamma < 1\\)) tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Policies and Value Functions  \n",
    "### Definitions:\n",
    "- **Policy (\\(\\pi\\))**: Mapping from states to action probabilities.\n",
    "  $$\n",
    "  \\pi(a \\mid s) = \\Pr\\{A_t = a \\mid S_t = s\\}\n",
    "  $$\n",
    "- **State-Value Function (\\(v_\\pi\\))**: Expected return from state \\(s\\) under \\(\\pi\\):\n",
    "  $$\n",
    "  v_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s]\n",
    "  $$\n",
    "- **Action-Value Function (\\(q_\\pi\\))**: Expected return from taking \\(a\\) in \\(s\\):\n",
    "  $$\n",
    "  q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s, A_t = a]\n",
    "  $$\n",
    "\n",
    "### Bellman Equation for \\(v_\\pi\\):\n",
    "$$\n",
    "v_\\pi(s) = \\sum_a \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) \\Bigl[ r + \\gamma v_\\pi(s') \\Bigr]\n",
    "$$\n",
    "\n",
    "**Example: Gridworld**  \n",
    "- States: Grid cells.  \n",
    "- Actions: Move N/S/E/W.  \n",
    "- Value function \\(v_\\pi\\) computed via Bellman equations (see Figure 3.2).\n",
    "\n",
    "---\n",
    "\n",
    "## 3.6 Optimal Policies and Value Functions  \n",
    "### Optimal Value Functions:\n",
    "- **Optimal State-Value Function (\\(v_*\\))**:\n",
    "  $$\n",
    "  v_*(s) = \\max_\\pi v_\\pi(s)\n",
    "  $$\n",
    "- **Optimal Action-Value Function (\\(q_*\\))**:\n",
    "  $$\n",
    "  q_*(s, a) = \\max_\\pi q_\\pi(s, a)\n",
    "  $$\n",
    "\n",
    "### Bellman Optimality Equations:  \n",
    "For \\(v_*\\):\n",
    "$$\n",
    "v_*(s) = \\max_a \\sum_{s', r} p(s', r \\mid s, a) \\Bigl[ r + \\gamma v_*(s') \\Bigr]\n",
    "$$\n",
    "For \\(q_*\\):\n",
    "$$\n",
    "q_*(s, a) = \\sum_{s', r} p(s', r \\mid s, a) \\Bigl[ r + \\gamma \\max_{a'} q_*(s', a') \\Bigr]\n",
    "$$\n",
    "\n",
    "**Example**:  \n",
    "- **Gridworld Optimal Policy**: Actions leading to highest values (see Figure 3.5).\n",
    "\n",
    "---\n",
    "\n",
    "## 3.7 Summary  \n",
    "- **MDPs** model sequential decision-making with states, actions, rewards, and dynamics.\n",
    "- **Value functions** (\\(v_\\pi\\), \\(q_\\pi\\)) and **Bellman equations** are central to solving MDPs.\n",
    "- **Optimal policies** maximize cumulative reward and satisfy Bellman optimality equations.\n",
    "- **Approximations** are often necessary due to computational limits in real-world tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises (Selected)\n",
    "1. **Exercise 3.1**: Design three MDP tasks (e.g., robot navigation, stock trading).\n",
    "2. **Exercise 3.8**: Compute returns for \\(\\gamma=0.5\\) and reward sequence \\([-1, 2, 6, 3, 2]\\).\n",
    "3. **Exercise 3.14**: Verify Bellman equation for Gridworldâ€™s center state.\n",
    "4. **Exercise 3.22**: Determine optimal policies for different \\(\\gamma\\) in a simple MDP.\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: [Chapter 4: Dynamic Programming](link) for solving MDPs with known dynamics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
